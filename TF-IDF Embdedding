#!/usr/bin/env python
import mysql.connector
from mysql.connector import Error
import numpy as np
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer

# ---------------- MySQL Configuration ----------------
MYSQL_CONFIG = {
    'host': 'localhost',
    'user': 'your_mysql_user',
    'password': 'your_mysql_password',
    'database': 'your_database_name'
}

TABLE_NAME = 'test_cases'
EMBEDDINGS_FILE = 'tfidf_embeddings.npy'
META_FILE = 'tfidf_metadata.pkl'   # To store record IDs and texts
VECTORIZER_FILE = 'tfidf_vectorizer.pkl'

def create_connection():
    """Create and return a MySQL database connection."""
    try:
        connection = mysql.connector.connect(**MYSQL_CONFIG)
        if connection.is_connected():
            return connection
    except Error as e:
        print("Error while connecting to MySQL:", e)
    return None

def fetch_test_cases():
    """Fetch all test cases from the database."""
    connection = create_connection()
    records = []
    if connection:
        try:
            cursor = connection.cursor(dictionary=True)
            cursor.execute(f"SELECT id, user_story, acceptance_criteria FROM {TABLE_NAME}")
            records = cursor.fetchall()
            print(f"Fetched {len(records)} test cases from the database.")
        except Error as e:
            print("Error fetching data:", e)
        finally:
            cursor.close()
            connection.close()
    return records

def combine_text(record):
    """Combine user_story and acceptance_criteria from a record."""
    return f"User Story: {record['user_story']} Acceptance Criteria: {record['acceptance_criteria']}"

def compute_and_save_tfidf_embeddings():
    test_cases = fetch_test_cases()
    if not test_cases:
        print("No test cases found. Exiting.")
        return

    texts = []
    metadata = []  # To store details such as id and combined text
    for tc in test_cases:
        combined = combine_text(tc)
        texts.append(combined)
        metadata.append({
            'id': tc['id'],
            'combined': combined,
            'user_story': tc['user_story'],
            'acceptance_criteria': tc['acceptance_criteria']
        })

    # Initialize a TF-IDF vectorizer.
    # max_features limits the dimensionality (adjust as needed).
    vectorizer = TfidfVectorizer(max_features=300)
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    # Convert the sparse matrix to a dense NumPy array.
    embeddings = tfidf_matrix.toarray().astype('float32')

    # Save the embeddings, metadata, and the vectorizer.
    np.save(EMBEDDINGS_FILE, embeddings)
    with open(META_FILE, 'wb') as f:
        pickle.dump(metadata, f)
    with open(VECTORIZER_FILE, 'wb') as f:
        pickle.dump(vectorizer, f)
    
    print(f"Saved {len(embeddings)} TF-IDF embeddings to {EMBEDDINGS_FILE}, metadata to {META_FILE}, and vectorizer to {VECTORIZER_FILE}.")

if __name__ == "__main__":
    compute_and_save_tfidf_embeddings()
