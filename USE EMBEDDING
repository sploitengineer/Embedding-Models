#!/usr/bin/env python
import mysql.connector
from mysql.connector import Error
import numpy as np
import pickle
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text  # needed for USE Lite
# ---------------- MySQL Configuration ----------------
MYSQL_CONFIG = {
    'host': 'localhost',
    'user': 'your_mysql_user',
    'password': 'your_mysql_password',
    'database': 'your_database_name'
}
TABLE_NAME = 'test_cases'
EMBEDDINGS_FILE = 'use_lite_embeddings.npy'
META_FILE = 'use_lite_metadata.pkl'

def create_connection():
    try:
        connection = mysql.connector.connect(**MYSQL_CONFIG)
        if connection.is_connected():
            return connection
    except Error as e:
        print("Error while connecting to MySQL:", e)
    return None

def fetch_test_cases():
    connection = create_connection()
    records = []
    if connection:
        try:
            cursor = connection.cursor(dictionary=True)
            cursor.execute(f"SELECT id, user_story, acceptance_criteria FROM {TABLE_NAME}")
            records = cursor.fetchall()
            print(f"Fetched {len(records)} test cases from the database.")
        except Error as e:
            print("Error fetching data:", e)
        finally:
            cursor.close()
            connection.close()
    return records

def combine_text(record):
    return f"User Story: {record['user_story']} Acceptance Criteria: {record['acceptance_criteria']}"

# Load the USE Lite model from TensorFlow Hub.
module_url = "https://tfhub.dev/google/universal-sentence-encoder-lite/2"
use_lite_model = hub.load(module_url)
# Create a KerasLayer for tokenization using the moduleâ€™s "tokenize" signature.
tokenizer_layer = hub.KerasLayer(module_url, signature="tokenize", output_shape=[None], dtype=tf.string)

def embed_text(texts):
    # Tokenize input texts (expects a list of strings).
    tokens = tokenizer_layer(texts)
    # Compute sequence lengths: count non-empty tokens per input.
    sequence_len = tf.reduce_sum(tf.cast(tf.not_equal(tokens, ""), tf.int32), axis=-1)
    # Prepare the input dictionary expected by the module.
    inputs = {"tokens": tokens, "sequence_len": sequence_len}
    embeddings = use_lite_model(inputs)
    return embeddings.numpy()

def compute_and_save_use_lite_embeddings():
    test_cases = fetch_test_cases()
    if not test_cases:
        print("No test cases found. Exiting.")
        return
    texts = []
    metadata = []
    for tc in test_cases:
        combined = combine_text(tc)
        texts.append(combined)
        metadata.append({
            'id': tc['id'],
            'combined': combined,
            'user_story': tc['user_story'],
            'acceptance_criteria': tc['acceptance_criteria']
        })
    embeddings = embed_text(texts)
    # USE Lite produces 512-d embeddings.
    embeddings = np.array(embeddings).astype('float32')
    np.save(EMBEDDINGS_FILE, embeddings)
    with open(META_FILE, 'wb') as f:
        pickle.dump(metadata, f)
    print(f"Saved {len(embeddings)} USE Lite embeddings to {EMBEDDINGS_FILE} and metadata to {META_FILE}.")

if __name__ == "__main__":
    compute_and_save_use_lite_embeddings()
