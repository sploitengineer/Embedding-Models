#!/usr/bin/env python
import mysql.connector
from mysql.connector import Error
import pickle
import numpy as np
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

# ---------------- MySQL Configuration ----------------
MYSQL_CONFIG = {
    'host': 'localhost',
    'user': 'your_mysql_user',
    'password': 'your_mysql_password',
    'database': 'your_database_name'
}
TABLE_NAME = 'test_cases'
EMBEDDINGS_FILE = 'doc2vec_embeddings.npy'
META_FILE = 'doc2vec_metadata.pkl'
DOC2VEC_MODEL_FILE = 'doc2vec_model.model'

def create_connection():
    try:
        connection = mysql.connector.connect(**MYSQL_CONFIG)
        if connection.is_connected():
            return connection
    except Error as e:
        print("Error while connecting to MySQL:", e)
    return None

def fetch_test_cases():
    connection = create_connection()
    records = []
    if connection:
        try:
            cursor = connection.cursor(dictionary=True)
            cursor.execute(f"SELECT id, user_story, acceptance_criteria FROM {TABLE_NAME}")
            records = cursor.fetchall()
            print(f"Fetched {len(records)} test cases from the database.")
        except Error as e:
            print("Error fetching data:", e)
        finally:
            cursor.close()
            connection.close()
    return records

def combine_text(record):
    return f"User Story: {record['user_story']} Acceptance Criteria: {record['acceptance_criteria']}"

def preprocess_text(text):
    return word_tokenize(text.lower())

def compute_and_save_doc2vec_embeddings():
    test_cases = fetch_test_cases()
    if not test_cases:
        print("No test cases found. Exiting.")
        return
    
    documents = []
    metadata = []
    for tc in test_cases:
        combined = combine_text(tc)
        tokens = preprocess_text(combined)
        documents.append(TaggedDocument(words=tokens, tags=[str(tc['id'])]))
        metadata.append({
            'id': tc['id'],
            'combined': combined,
            'user_story': tc['user_story'],
            'acceptance_criteria': tc['acceptance_criteria']
        })
    
    # Train Doc2Vec model
    model = Doc2Vec(vector_size=100, min_count=1, epochs=40)  # Adjust vector_size as needed.
    model.build_vocab(documents)
    model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)
    model.save(DOC2VEC_MODEL_FILE)
    
    # Infer embeddings for each document.
    embeddings = []
    for doc in documents:
        emb = model.infer_vector(doc.words)
        embeddings.append(emb)
    
    embeddings = np.array(embeddings).astype('float32')
    np.save(EMBEDDINGS_FILE, embeddings)
    with open(META_FILE, 'wb') as f:
        pickle.dump(metadata, f)
    
    print(f"Saved {len(embeddings)} Doc2Vec embeddings to {EMBEDDINGS_FILE}, metadata to {META_FILE}, and model to {DOC2VEC_MODEL_FILE}.")

if __name__ == "__main__":
    compute_and_save_doc2vec_embeddings()
